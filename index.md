---
layout: default
title: Home
---
<b>Ye Yuan</b>

<p><a href="https://hybrid.eecs.berkeley.edu/~yeyuan/cv_yeyuan_2015.pdf">CV, <a href="https://scholar.google.com/citations?user=Jhj7LZUAAAAJ&hl=en">Citations</a></p>

<p>Postdoctoral Researcher<br  />
<a href="http://hybrid.eecs.berkeley.edu/">Hybrid Systems Lab</a> and <a href="http://bair.berkeley.edu/students.html">Berkeley Artificial Intelligence Research Lab</a><br  />
Department of Electrical Engineering and Computer Sciences<br  />
UC Berkeley <br  />
Advisor: <a href="http://www.eecs.berkeley.edu/~tomlin">Professor Claire J. Tomlin</a></p>


<!--<b>News:</b>-->


<!--<p><small>[Jan 20, 2016] Our paper: “Network identifiability from intrinsic noise,” was accepted by IEEE Transactions on Automatic Control. </small></p>-->


<p> <b>Working Paper:</b> <a href="https://hybrid.eecs.berkeley.edu/~yeyuan/pb.pdf">On Powerball Method.</a> (Posted: 3/14/16) arxiv, <a href="https://github.com/mli/powerball"> code</a>. </p>

<p><small> We propose a new method called Powerball to accelerate the convergence of optimization algorithms. This method adds a power coefficient γ ∈ (0, 1) to the gradient during optimization. We prove that the Powerball method can achieve ε accuracy for strongly convex functions by using O((1−γ)^(−1)ε^(γ−1)) iterations. We also demonstrate that the Powerball method speeds up the convergence of both gradient descent and L-BFGS on multiple real datasets up to 10 times.</small></p>

<!--<p><font color="red">I am in the job market, research and teaching statements are available upon request. </font></p>-->


<b>Contact:</b>

<p><small>Office: 307 Cory Hall <br  />
<!--(most of the time) or Desk 36, 732 Sutardja Dai Hall<br  />-->
Email: yy311@berkeley.edu.</small></p>



<span class="footercued">
Updated on 3/2016.<br />
<span>


<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?u=7Veh&d=yguR5_G3NUuhN_gFSGtzaYE7LKn1yFCyVuc9_ytJA_o"></script>
